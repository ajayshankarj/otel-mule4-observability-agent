= Mule 4 OpenTelemetry (OTel) Agent Extension
// Document header
Rick Bansal <rick.bansal@mulesoft.com>
:revnumber: 1.0.92-SNAPSHOT
:revdate: Feb. 19, 2022
:revremark: Initial Draft
:doctype: book
:icons: font
:toc: left
:imagesdir: ./Images
:keywords: Mule, MuleSoft, Observability, OpenTelemetry, OTel, Tracing, Instrumentation, Distributed

// The following pass through will align the images and their titles
ifndef::env-github[]
++++
<style>
  .imageblock > .title {
    text-align: inherit;
    margin-top: 10px;
  }
</style>
++++
endif::[]

ifdef::env-github[]
:caution-caption: :fire:
:important-caption: :heavy_exclamation_mark:
:note-caption: :information_source:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]



// Custom attributes
:wc3-trace-context-url: https://w3c.github.io/trace-context/

// Document body
== Introduction

The Mule 4 OTel Agent is a custom MuleSoft *extension* for instrumenting MuleSoft applications to export tracing 
specific *telemetry* data to any OpenTelemetry compliant collector.  Using the agent now allows Mule applications 
to be insightful participants in the overall *distributed trace*.

== The Why on the Mule 4 OTel Agent

The motivation behind developing this extension is predicated on *two primary concerns* as it 
relates to enterprise software:

* Composability
* Observability

=== Composability

.What exactly is composability and why does it matter?

Well, as you might imagine, there are plenty of 
theoretical, complex and technical answers to this question - just Google it to get a list of the numerous publications
on the topic. Since this is not a technical article on the subject of composability, we'll take a much more modest view of it.

So, in really simple terms, *_composability_* is the concept of building stand alone software *composed* of 
other stand alone software, in a plug-and-play manner (see figure <<Composable-enterprise-app-1>> below) and it matters because 
enterprises who adopt composability as a core IT practice can achieve much greater *agility* on delivering new and/or enhanced 
solutions for business in the face of rapid and ever changing market conditions - does COVID ring a bell?  

[#Composable-enterprise-app-1]
image::Composable-enterprise-app-1.png[600, 600, title="Example of a Composite Application", align="center"]

Basically, the practice of composability is a great way for an enterprise to *protect* and *grow* overall *revenue* in 
the face of both expected and unexpected change. Do you know know when or what the next crisis will be?  Exactly...

=== Composable Enterprise

https://www.gartner.com/smarterwithgartner/gartner-keynote-the-future-of-business-is-composable[Gartner] 
defines a *_Composable Enterprise_* as an organization that can innovate and adapt to changing
business needs through the assembly and combination of packaged business capabilities.

[NOTE] 
====
Gartner's definition of composable business operates on four basic principles: 

* More speed through *discovery*.
* Greater agility through *modularity*.
* Better leadership through *orchestration*.
* Resilience through *autonomy*.
====

Composability must be important because it has its own Gartner definition, right?

==== So how long before we have true composable enterprises? 
 
From a purist standpoint (i.e., based on the Gartner definition), 
who knows - maybe never.  However, from a practical perspective the "messy" composable enterprise is *already here*, has been 
for a while and it's quickly getting more "pure" over time.

For example,

* A typical enterprise supports over *900 applications* and the number is *growing*, not shrinking.
** Growth is happening because of:
*** Accelerated implementation of *digital transformation strategies* with a cloud-first approach.
*** Rapid adoption of a *microservices* architecture paradigm.

* Typically, no single enterprise application handles a business transaction.
** A typical *business transaction traverses over 35 different systems/applications* from start to finish.
*** These systems/applications are often on a variety of *disparate* and independent *technologies* stacks - both legacy and modern.
*** These systems are often a combination of on-prem or hosted *packaged* applications (e.g., SAP ERP, Oracle HCM, Manhattan SCM, 
etc.), *custom* coded applications and *SaaS* applications (e.g., Salesforce, NetSuite, Workday, etc.)

So as you can see, the composable enterprise already exists and will, rapidly, become more composable over time, especially,
with the support of companies like MuleSoft, products like the Anypoint Platform and methodologies like API-Led Connectivity.

image::API-Led-1.png[title="API Led to Help Solve for Composability", align="center"]


//image::MuleSoft-Solution-Composability.png[title="API Led for Composability", align="center"]



=== Observability

[quote]
Wikipedia defines *observability* as a measure of how well internal states of a system can be inferred from knowledge of 
its external outputs.  As it relates specifically to software, observability is the ability to collect *data about program 
execution, internal states of modules, and communication between components*.  This corpus of collected data is also referred 
to as *telemetry*.

Another way of looking at observability is having the capacity to introspect, in real-time, complex multi-tiered architectures to 
better answer the following when things so sideways:

* Where and why is it broken?
* Where and why is it slow?

Then, using the gathered observability insights to quickly fix what's broken and speedup what's slow.

[NOTE]
====
However, I think a *more important* consideration for observability is an answer to following:

* How can I *proactively* protect against failure and poor performance?
====

==== Observability Trinity

The obtainment of true observability relies upon 3 core pillars.

image::Pillars-of-Observability.png[600, 600, title="The 3 Pillars of Observability", align="center"]

===== Metrics
A *_metric_* is a value that expresses some information about a system. Metrics are 
usually represented as counts or measures, and are often aggregated or calculated over a period of time. Additionally, metrics 
are often structured as _<name, value>_ pairs that provide useful behavioral details at both the micro-level and the macro-level 
such as the following:

.Example Metrics
|===
| *Micro-level metrics*           | *Macro-level metrics*
| Memory utilization per service  | Average response time per service
| CPU utilization per service     | Throughput rate per service
| Thread count                    | Failure rate per service
| ...                             | ...
|===

image::Macro-Micro-Metrics.png[title="Micro-level and Macro-level Metrics", align="center"]

===== Logs
A *log* is an immutable, time-stamped text or binary record, either structured (recommended) or unstructured, potentially including 
metadata. The log record is generated by application code in response to an event (e.g., an error condition) which has occurred
during program execution.

.Example of a structured log record
[literal]
....
[02-22 08:02:50.412] ERROR OnErrorContinueHandler [ [MuleRuntime].uber.18543: [client-id-enforcement-439118-order-api-spec-main].439118-client-id-enforcement.CPU_LITE @5b1b413e] [event: d46fe7b0-93b5-11ec-b9b6-02d407c48f42]: 
Root Exception stack trace:
org.mule.runtime.api.el.ExpressionExecutionException: Script 'atributes.headers ' has errors:
...
....

.Example of a unstructured log record
 'hello world'

===== Traces
A *single trace* is an event which shows the activity for a transaction or request as it flows through an individual application. 
Whereas, a *distributed trace* is an aggregation of one or more single traces when the transaction spans across multiple  
application, network, security and environment boundaries.  For example, a distributed trace may be initiated when someone presses a 
button to start an action on a website - such as purchasing a product.  In this case, the distributed trace will represent calls made 
between all of the downstream services (e.g. Inventory, Logistics, Payment, etc.) that handled the chain of requests initiated by 
the initial button press.

*Distributed tracing* is the methodology implemented by tracing tools to generate, follow, analyze and debug a distributed trace.
Generation of a distributed trace is accomplished by tagging the transaction with a unique identifier and propagating that identifier
through the chain of systems involved in the transaction.  This process is also referred to as *trace context propagation*.


Traces are a critical part of observability, as they provide context for other telemetry. For example, traces can help define 
which metrics would be most valuable in a given situation, or which logs are relevant to a particular issue.

image::Distributed-Trace-Example.png[title="Example of a Distributed Trace", align="center"]

==== Why is observability important?  

The notion of observability is very important to IT organizations because when a business transaction fails or performs 
poorly within their application network, the team needs the ability to quickly *triage* and *remediate* the root cause 
before there is any significant impact on revenue.  

Many IT organizations have and continue to rely upon commercial Application Performance Monitoring (APM) tools (e.g., AppDynamics, 
Dynatrace, New Relic, CA APM, ...) to help them in this regard.  While useful, these commercial tools have struggled in the past
to provide complete visibility into the overall distributed trace as they deploy vendor specific agents to collect and forward 
their telemetry.

I state "_struggled in the past_" because many APM vendors are now starting to embrace and support open source projects like 
https://opentelemetry.io/docs/reference/specification/overview/[OpenTelemetry] for vendor-agnostic instrumentation agent 
implementations and standards such as {wc3-trace-context-url}/[W3C Trace Context] for context propagation 
to help them fill in the "holes".

==== So what do composability and observability have to do with each other?  

Hopefully, the answer is obvious but as enterprise applications become more and more composable, that is, as enterprises move 
towards embracing composability as an architectural pattern, the need for observability becomes greater; however, the capacity 
for implementing observability becomes harder unless there is comprehensive observability strategy and solution in place.

=== Solving for Observability

MuleSoft has traditionally been a very strong player in two aspects of the Observability Trinity - Metrics and Logs.  Anypoint 
Monitoring provides considerable support and functionality for these two observability data sources.  However, there has been a gap
in the support for tracing (single traces and distributed traces).  This limitation within the current offering is the inspiration
behind the development of the custom extension. 

Together, *Anypoint Monitoring and Mule 4 Otel Agent* offer a more comprehensive and robust observability solution and should be 
part of an enterprise's overall observability solution.

image::Solving-for-observability.png[800, 800, title="Observability = Anypoint Monitoring + Otel Mule 4 Agent", align="center"]


== The What on the Mule 4 OTel Agent

Now that we done a comprehensive walkthrough on the motivation for developing the Mule 4 OTel Agent custom extension, let's dig 
a bit deeper into some of the internals of extension.  We'll start off by diving into the core technology the extension relies 
upon to accomplish its tasks - _OpenTelemetry_ then discuss the WC3 Trace Context specification and finish off with details on the
extension's architecture.

=== OpenTelemetry (OTel)

[quote, OpenTelemetry, 'https://opentelemetry.io']
OpenTelemetry *is a set* of APIs, SDKs, tooling and integrations that are designed for the creation and management 
of *telemetry data* such as traces, metrics, and logs. The project provides a *vendor-agnostic* implementation that 
can be configured to send telemetry data to the backend(s) of your choice.

IMPORTANT: OpenTelemetry *is not* an observability back-end.  Instead, it supports exporting data to a variety of open-source 
(e.g., Jaeger, Prometheus, etc.) and commercial back-ends (e.g., Dynatrace, New Relic, Grafana, etc.). 

As noted above, OpenTelemetry is a *framework* which provides a single, vendor-agnostic solution with the purpose 
of standardizing the generation, emittance, collection, processing and exporting of telemetry data in support of observability.
OpenTelemetry was established in 2019 as an open source project and is spearheaded by the CloudNative Computing Foundation (CNCF).

[NOTE]
====
In 2019, the https://opencensus.io/[OpenCensus] and https://opentracing.io/[OpenTracing] projects merged into OpenTelemetry. 
Currently, OpenTelemetry is at the "*incubating*" https://github.com/cncf/toc/blob/main/process/graduation_criteria.adoc[maturity 
level] (up from "sandbox" level a year back) and is one of the most popular projects across the CNCF landscape.
====

==== OpenTelemetry Reference Architecture

Being a CNCF supported project, it's no surprise the architecture of OpenTelemetry is cloud friendly - which also implies that 
it is friendly to all distributed environments. While there are various aspects to the overall OpenTelemetry framework (e.g.,
API, SDK, Signals, Packages, Propagators, Exporters, etc.), the functional architecture is relatively simple with regard to 
client-side implementations as seen in the diagram below.

image::Otel-Ref-Arch-2-shadowing.png[800, 800, title="OpenTelemetry Reference Architecture", align="center"]

On the client side (e.g., the Mule application), there are really only two OpenTelemetry components which are used and one is 
optional:

OpenTelemetry Library::
* OpenTelemtry API
* OpenTelemtry SDK

OpenTelemetry Collector::
* _[Optional]_

Below is a brief description of these client-side components.  

===== OpenTelemetry API
The OpenTelemetry API is an *abstracted implementation* of data types and  non-operational methods for generating and 
correlating tracing, metrics, and logging data.  Functional implementations of the API are language specific.

===== OpenTelemetry SDK
The OpenTelemetry SDK is a *language specific implementation* (e.g., Java, Ruby, C++, ...) of the abstracted OpenTelemetry API. 
Here is a https://opentelemetry.io/docs/instrumentation/[list] of the currently supported languages.

===== OpenTelemetry Collector
The OpenTelemetry Collector is a *vendor-agnostic proxy* that can receive, process, and export telemetry data. It supports 
receiving telemetry data in multiple formats (e.g., OTLP, Jaeger, Prometheus, as well as many commercial/proprietary tools) 
and sending data to one or more back-ends. It also supports processing and filtering telemetry data before it gets exported. 

[NOTE]
You can find more details on the API and SDK https://opentelemetry.io/docs/reference/specification/#table-of-contents[here] 
and on the Collector https://opentelemetry.io/docs/collector/[here].

=== WC3 Trace Context

==== Trace Context
The WC3 Trace Context https://w3c.github.io/trace-context/[specification] defines a universally agreed-upon format for the exchange of 
trace context propagation data - referred to as *_trace context_*. Trace context solves the problems typically associated with distributed
tracing by:

* Providing a unique identifier for individual traces and requests, allowing trace data of multiple providers to be linked together.

* Providing an agreed-upon mechanism to forward vendor-specific trace data and avoid broken traces when multiple tracing tools participate 
in a single transaction.

* Providing an industry standard that intermediaries, platforms, and hardware providers can support.

==== Trace Context Headers
Trace context is split into two individual propagation fields supporting interoperability and vendor-specific extensibility:

.`traceparent`
Describes the position of the incoming request in its trace graph in a portable, fixed-length format. Every tracing tool *MUST* properly 
set traceparent even when it only relies on vendor-specific information in `tracestate`

.`tracestate`
Extends `traceparent` with vendor-specific data represented by a set of name/value pairs. Storing information in `tracestate` is *optional*.

*Tracing tools* can provide two levels of compliant behavior interacting with trace context:

* At a minimum they *MUST* propagate the `traceparent` and `tracestate` headers and guarantee traces are not broken. This behavior is also 
referred to as _forwarding a trace_.

* In addition they *_MAY_* also choose to participate in a trace by modifying the `traceparent` header and relevant parts of the `tracestate` 
header containing their proprietary information. This is also referred to as _participating in a trace_.

==== `traceparent` HTTP Header Details

The `traceparent` header represents the incoming request in a tracing system in a common format, understood by all vendors. 

The header has *4 constituent parts*, where each part is separated by a `-`:

* `version` - header version; currently the version number is `00`
* `trace-id` - is the *unique 16-byte ID* of a distributed trace through a system. 
* `parent-id` - is the 8-byte ID of this request as known by the caller (sometimes known as the `span-id`, where a span is the execution 
                of a client request).
* `trace-flags` - tracing control flags; current version (`00`) only supports the `sampled` flag (`01`)

image::traceparent-header.png[700, 700, title="`traceparent` HTTP Header ", align="center"]

=== MuleSoft OpenTelemetry Agent Architecture

As mentioned earlier, the primary purpose of the Mule 4 OTel Agent extension is to facilitate the participation of Mule applications in 
distributed tracing activities. To accomplish its goal, the extension relies upon three primary frameworks:

. MuleSoft Java SDK
. MuleSoft Server Notifications
. OpenTelemetry

image::Agent-Arch.png[700, 700, title="Mule Agent Architecture", align="center"]

==== MuleSoft Java SDK
In Mule 4, extending the product is done by developing custom extensions via a MuleSoft furnished Java SDK. The comprehensive framework
allows external developers to build add-on functionality in the same manner as Mule engineers build Mule supplied components and connectors.
While we won't get into the details of the framework or how to develop a custom extension, the graphic below depicts the basic structure of
an extension based on the https://docs.mulesoft.com/mule-sdk/1.1/module-structure[Module Model]. 

image::mule-extension-model.png[800, 800, title="The Extension Module Model Structure", align="center"] 

==== MuleSoft Server Notifications
Mule provides an internal https://docs.mulesoft.com/mule-runtime/4.4/mule-server-notifications[notification mechanism] 
that can be used to access changes which occur on the Mule Server, such as adding a flow component, the start or end of a message processor, a
failing authorization request and many other https://docs.mulesoft.com/mule-runtime/4.4/mule-server-notifications#notification-interfaces[changes].
These notifications can be subscribed to by "listeners" either programmatically or by using the `<notifications>` element in a Mule
configuration file.

.Example of subscribing to notifications programmatically
[source%nowrap, java]
----
notificationListenerRegistry.registerListener(new MuleMessageProcessorNotificationListener(otelMuleNotificationHandler));

notificationListenerRegistry.registerListener(new MulePipelineNotificationListener(otelMuleNotificationHandler));
----

.Example of subscribing to notifications using the `<notification>` element
[source, xml]
----
<notifications>
	<notification event="PIPELINE-MESSAGE"/>
	<notification event="MESSAGE-PROCESSOR"/>
	<notification-listener ref="_muleMessageProcessorNotificationListener"/>
	<notification-listener ref="_muleFlowNotificationListener"/>
</notifications>
----

The agent takes advantage of the notification framework and in particular relies upon these two notification interfaces:

* `PipelineMessageNotificationListener`
** Start and End of a flow 

* `MessageProcessorNotificationListener`
** Start and End of a message processor

==== OpenTelemetry
The only component Mule uses from the OpenTelemetry framework is the OTel 

== The How on the Mule 4 OTel Agent



=== Installation Guideline

=== Configuration of the Agent

=== Example Scenario

=== Example Output - Dynatrace Backend

[bibliography]
== References
* {wc3-trace-context-url}[wc3.org: "Trace Context Draft Recommendation"]
* {wc3-trace-context-url}/#dfn-distributed-traces[wc3.org: "Distributed Traces"]
* https://opentelemetry.io/[opentelemetry.io]
* https://lightstep.com/observability/[Lightstep: "Observability: A complete overview for 2021"]
* https://www.dynatrace.com/resources/ebooks/observability-and-beyond-for-the-enterprise-cloud/[Dynatrace e-book:
"Observability and Beyond for the Enterprise Cloud"]
* https://www.splunk.com/en_us/form/beginners-guide-to-observability.html[Splunk e-book: "A Beginner's Guide to Observability"]
* https://docs.mulesoft.com/monitoring/[MuleSoft: "Anypoint Monitoring Overview"]